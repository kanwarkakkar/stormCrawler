storm-crawler-elasticsearch
===========================

A collection of resources for [Elasticsearch](https://www.elastic.co/products/elasticsearch):
* [IndexerBolt](https://github.com/DigitalPebble/storm-crawler/blob/master/external/elasticsearch/src/main/java/com/digitalpebble/stormcrawler/elasticsearch/bolt/IndexerBolt.java) for indexing documents fetched with StormCrawler
* [Spout](https://github.com/DigitalPebble/storm-crawler/blob/master/external/elasticsearch/src/main/java/com/digitalpebble/stormcrawler/elasticsearch/persistence/ElasticSearchSpout.java) and [StatusUpdaterBolt](https://github.com/DigitalPebble/storm-crawler/blob/master/external/elasticsearch/src/main/java/com/digitalpebble/stormcrawler/elasticsearch/persistence/StatusUpdaterBolt.java) for persisting URL information in recursive crawls
* [MetricsConsumer](https://github.com/DigitalPebble/storm-crawler/blob/master/external/elasticsearch/src/main/java/com/digitalpebble/stormcrawler/elasticsearch/metrics/MetricsConsumer.java) 

as well as examples of crawl and injection topologies.

We also have resources for [Kibana](https://www.elastic.co/products/kibana) to build basic real-time monitoring dashboards for the crawls, such as the one below.

![bla](https://pbs.twimg.com/media/CR1-waVWEAAh0u4.png)

Getting started
---------------------

We'll assume that Elasticsearch and Kibana are installed and running on your machine. You'll also need Java, Maven and Storm installed.

First, go to the root of the project and compile it with `mvn clean install` then move to the _external/elasticsearch_ directory and build an uber jar with `mvn clean install -P bigjar`.

Then we run the script `ES_IndexInit.sh`, which creates 2 indices : one for persisting the status of URLs (_status_) and a template mapping for persisting the Storm metrics (for any indices with a name matching _metrics*_). A third index (_index_) for searching the documents fetched by stormcrawler will be created automatically by the topology, you should probably tune its mapping later on.

We can inject the seed URLs into the _status_ index by putting them in a text file with one URL per line e.g.

`echo 'http://www.theguardian.com/newssitemap.xml' > seeds.txt`

then call the ESSeedInjector topology with 

`storm jar target/storm-crawler-elasticsearch-*-SNAPSHOT.jar com.digitalpebble.stormcrawler.elasticsearch.ESSeedInjector . seeds.txt -local -conf es-conf.yaml -ttl 60`

The injection topology will terminate by itself after 60 seconds. 

You should then be able to see the seeds in the [status index](http://localhost:9200/status/_search?pretty).

You are almost ready to launch the crawl. First you'll need to create a _crawler-conf.yaml_ configuration file. The [example conf generated by the archetype](https://github.com/DigitalPebble/storm-crawler/blob/master/archetype/src/main/resources/archetype-resources/crawler-conf.yaml) should be a good starting point. If you do use it, don't forget to copy the files from the [resources dir](https://github.com/DigitalPebble/storm-crawler/tree/master/archetype/src/main/resources/archetype-resources/src/main/resources) as well.

Since we are about to deal with sitemap files, it would be a good idea to add at least \:

```
sitemap.sniffContent: true
http.content.limit: -1
parser.emitOutlinks: false
```

so that the parser for the sitemap files detects them automatically and that the fetcher does not trim the content, which might cause the parser to fail. This configuration will also prevent the crawl from discovery outlinks from the HTML pages listed in the sitemaps.

As a general good practice, you should also specify the _http.agent.*_ configurations so that the servers you fetch from can identify you.

When it's done run 

`storm jar target/storm-crawler-elasticsearch-*-SNAPSHOT.jar com.digitalpebble.stormcrawler.elasticsearch.ESCrawlTopology -local -conf es-conf.yaml -conf crawler-conf.yaml`
  
to start the crawl. You can remove `-local` to run the topology on a Storm cluster.

Kibana
---------------------

In [Kibana](http://localhost:5601/#/settings/objects),

1. create the Index Patterns `status` and `metrics`: `Settings > Indices > Add New`, enter `status` as `Index name or pattern`, and press `Create`. Repeat these steps also for `metrics`.
2. to upload the dashboard configurations do `Settings > Objects > Import` and select the file `kibana/status.json`.  Then go to `Dashboard`, click on `Loads Saved Dashboard` and select `Crawl Status`. You should see a table containing a single line _DISCOVERED 1_.
3. repeat the operation with the file `kibana/metrics.json`.

The [Metrics dashboard](http://localhost:5601/#/dashboard/Crawl-metrics) in Kibana can be used to monitor the progress of the crawl.

#### Per time period metric indices (optional)
Note, a second option for the _metrics_ index is available: the use of per time period indices. This best practice is [discussed on the Elastic website](https://www.elastic.co/guide/en/elasticsearch/guide/current/time-based.html).

The crawler config YAML must be updated to use either the day or month Elasticsearch metrics consumer, as shown below with the per day indices consumer:
```
 #Metrics consumers:
    topology.metrics.consumer.register:
       - class: "org.apache.storm.metric.LoggingMetricsConsumer"
         parallelism.hint: 1
       - class: "com.digitalpebble.stormcrawler.elasticsearch.metrics.IndexPerDayMetricsConsumer"
         parallelism.hint: 1
```

Archetype
---------------------

Instead of having to copy the default config files, you will probably find it easier to use the archetype \:

`mvn archetype:generate -DarchetypeGroupId=com.digitalpebble.stormcrawler -DarchetypeArtifactId=storm-crawler-archetype -DarchetypeVersion=1.3`

Once the directory has been created and populated, copy the es-conf.yaml and flux files to it. You can then edit the pom.xml and add

```
		<dependency>
			<groupId>com.digitalpebble.stormcrawler</groupId>
			<artifactId>storm-crawler-elasticsearch</artifactId>
			<version>1.3</version>
		</dependency>
```

Edit the *-conf.yaml files as you see fit, compile with Maven and run the crawl topology with \:

`storm jar target/*-1.0-SNAPSHOT.jar  org.apache.storm.flux.Flux --local es-crawler.flux`

You can use the injection topology in a similar way \:

`storm jar target/*-1.0-SNAPSHOT.jar  org.apache.storm.flux.Flux --local es-injector.flux`








